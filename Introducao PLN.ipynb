{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech - Portugues\n",
    "\n",
    "Identificando o que é determinada parte do discurso em lingua portuguesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.pt.Portuguese at 0x11abd1850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pln = spacy.load('pt')\n",
    "pln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria um documento que nada mais é do que um objeto spacy que contém os Tokens e POS. Verificar em: https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documento = pln('Estou aprendendo processamento de linguagem natural, curso em Curitiba.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estou AUX\n",
      "aprendendo VERB\n",
      "processamento NOUN\n",
      "de ADP\n",
      "linguagem NOUN\n",
      "natural ADJ\n",
      ", PUNCT\n",
      "curso NOUN\n",
      "em ADP\n",
      "Curitiba PROPN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in documento:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemma: raiz da palavra \n",
    "\n",
    "pos: parte da fala\n",
    "\n",
    "tag: informacoes morfológicas, ex se o verbo está no passado\n",
    "\n",
    "dep: dependencia sintática,\n",
    "\n",
    "shape: formato(maiúsculo, minúsculo, dígitos)\n",
    "\n",
    "alpha: se é alphabético\n",
    "\n",
    "stop: se é stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estou Estou AUX aux Xxxxx True True\n",
      "aprendendo aprender VERB ROOT xxxx True False\n",
      "processamento processamento NOUN obj xxxx True False\n",
      "de de ADP case xx True True\n",
      "linguagem linguagem NOUN nmod xxxx True False\n",
      "natural natural ADJ amod xxxx True False\n",
      ", , PUNCT punct , False False\n",
      "curso cursar NOUN appos xxxx True False\n",
      "em em ADP case xx True True\n",
      "Curitiba Curitiba PROPN nmod Xxxxx True False\n",
      ". . PUNCT punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in documento:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition\n",
    "\n",
    "Reconhecer entidades dentro de um texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM ORG\n",
      "Estados Unidos LOC\n",
      "Brasil LOC\n",
      "São Paulo LOC\n"
     ]
    }
   ],
   "source": [
    "texto = 'A IBM é uma empresa dos Estados Unidos voltada para a área de informática. Sua sede no Brasil fica em São Paulo e a receita em 2018 foi de aproximadamente 328 bilhões de reais.'\n",
    "\n",
    "\n",
    "documento = pln(texto)\n",
    "\n",
    "for entidade in documento.ents:\n",
    "    print(entidade.text, entidade.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinaldo Maciel PER\n",
      "Brasil LOC\n"
     ]
    }
   ],
   "source": [
    "texto = 'Reinaldo Maciel é um cara muito conhecido no Brasil.'\n",
    "documento = pln(texto)\n",
    "\n",
    "for entidade in documento.ents:\n",
    "    print(entidade.text, entidade.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade entre palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reinaldodeoliveiramaciel/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8258470163434681"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = pln('olá')\n",
    "p2 = pln('oi')\n",
    "p3 = pln('ou')\n",
    "\n",
    "p1.similarity(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reinaldodeoliveiramaciel/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.556686068341704"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.similarity(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reinaldodeoliveiramaciel/opt/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5912281781129909"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2.similarity(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
